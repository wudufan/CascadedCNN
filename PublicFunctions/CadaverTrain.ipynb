{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import caffe\n",
    "import numpy as np\n",
    "import ReadFromCavaderData\n",
    "import NNDenoise\n",
    "import GenerateNetwork\n",
    "import os\n",
    "import h5py\n",
    "import sys\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make patched images\n",
    "# src - the noisy input image list \n",
    "# refs - the residue image list (noisy - noiseless)\n",
    "# indices - indicate wich layers to be included in the patching, None for all layers\n",
    "# nPatchesPerLayer - number of patches per layer\n",
    "# patchSize - size of patch\n",
    "def PatchingImgs(srcs, refs, indices=None, nPatchesPerLayer=100, patchSize=[40,40]):\n",
    "    if indices is None:\n",
    "        indices = range(0,len(srcs))\n",
    "    \n",
    "    print 'Patching:'\n",
    "    imgPatches = np.zeros([len(indices), nPatchesPerLayer, 2, patchSize[0], patchSize[1]])\n",
    "    for iLayer in range(0, len(indices)):\n",
    "        if iLayer%10 == 0:\n",
    "            print '%d...'%iLayer,\n",
    "            sys.stdout.flush()\n",
    "        startPosX = np.random.randint(0, srcs[0].shape[0]-patchSize[0], [nPatchesPerLayer])\n",
    "        startPosY = np.random.randint(0, srcs[0].shape[1]-patchSize[1], [nPatchesPerLayer])\n",
    "        endPosX = startPosX + patchSize[0]\n",
    "        endPosY = startPosY + patchSize[1]\n",
    "        for iPatch in range(0, nPatchesPerLayer):\n",
    "            imgPatches[iLayer, iPatch, 0, ...] = srcs[indices[iLayer]][startPosX[iPatch]:endPosX[iPatch], startPosY[iPatch]:endPosY[iPatch]]\n",
    "            imgPatches[iLayer, iPatch, 1, ...] = refs[indices[iLayer]][startPosX[iPatch]:endPosX[iPatch], startPosY[iPatch]:endPosY[iPatch]]\n",
    "    print 'Done'\n",
    "    \n",
    "    # reshape\n",
    "    imgPatches = imgPatches.reshape([len(indices) * nPatchesPerLayer, 2, patchSize[0], patchSize[1]])\n",
    "    \n",
    "    # random transform\n",
    "    print 'Transforming...', \n",
    "    for iPatch in range(0,imgPatches.shape[0]):\n",
    "        if np.random.rand(1) < 0.5:\n",
    "            imgPatches[iPatch,...] = imgPatches[iPatch,:,::-1,:]\n",
    "        if np.random.rand(1) < 0.5:\n",
    "            imgPatches[iPatch,...] = imgPatches[iPatch,:,:,::-1]\n",
    "    print 'Done'\n",
    "    \n",
    "    # random the patches \n",
    "    inds = range(0, imgPatches.shape[0])\n",
    "    random.shuffle(inds)\n",
    "    imgPatches = imgPatches[inds, ...]\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return imgPatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do almost the same thing as PatchingImgs, the input is the two layers including the original noisy image\n",
    "# srcs - the input\n",
    "# srcs2 - the original noisy image\n",
    "# refs - the residue (noisy - noiseless)\n",
    "def PatchingImgs2(srcs, srcs2, refs, indices=None, nPatchesPerLayer=100, patchSize=[40,40]):\n",
    "    if indices is None:\n",
    "        indices = range(0,len(srcs))\n",
    "    \n",
    "    print 'Patching:'\n",
    "    imgPatches = np.zeros([len(indices), nPatchesPerLayer, 3, patchSize[0], patchSize[1]])\n",
    "    for iLayer in range(0, len(indices)):\n",
    "        if iLayer%10 == 0:\n",
    "            print '%d...'%iLayer,\n",
    "            sys.stdout.flush()\n",
    "        startPosX = np.random.randint(0, srcs[0].shape[0]-patchSize[0], [nPatchesPerLayer])\n",
    "        startPosY = np.random.randint(0, srcs[0].shape[1]-patchSize[1], [nPatchesPerLayer])\n",
    "        endPosX = startPosX + patchSize[0]\n",
    "        endPosY = startPosY + patchSize[1]\n",
    "        for iPatch in range(0, nPatchesPerLayer):\n",
    "            imgPatches[iLayer, iPatch, 0, ...] = srcs[indices[iLayer]][startPosX[iPatch]:endPosX[iPatch], startPosY[iPatch]:endPosY[iPatch]]\n",
    "            imgPatches[iLayer, iPatch, 1, ...] = srcs2[indices[iLayer]][startPosX[iPatch]:endPosX[iPatch], startPosY[iPatch]:endPosY[iPatch]]\n",
    "            imgPatches[iLayer, iPatch, 2, ...] = refs[indices[iLayer]][startPosX[iPatch]:endPosX[iPatch], startPosY[iPatch]:endPosY[iPatch]]\n",
    "    print 'Done'\n",
    "    \n",
    "    # reshape\n",
    "    imgPatches = imgPatches.reshape([len(indices) * nPatchesPerLayer, 3, patchSize[0], patchSize[1]])\n",
    "    \n",
    "    # random transform\n",
    "    print 'Transforming...', \n",
    "    for iPatch in range(0,imgPatches.shape[0]):\n",
    "        if np.random.rand(1) < 0.5:\n",
    "            imgPatches[iPatch,...] = imgPatches[iPatch,:,::-1,:]\n",
    "        if np.random.rand(1) < 0.5:\n",
    "            imgPatches[iPatch,...] = imgPatches[iPatch,:,:,::-1]\n",
    "    print 'Done'\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return imgPatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train denoising net for low dose challenge data\n",
    "# indices - the indices of the data to be used\n",
    "# depth - depth of the network\n",
    "# outputPath - outputPath of the trained networks\n",
    "# max_iter - max iteration numbers of the training\n",
    "# nChannel - 1 for single layer input, 2 for input including original noisy image\n",
    "# patchSize - size of patch, assuming square\n",
    "# patchPath - the path of the training datasets, looking for the hd5f index file 'trainingList.txt'\n",
    "def TrainLowDoseChallenge(indices, depth, outputPath, max_iter=55000, nChannel=1, patchSize=40,\n",
    "                          patchPath='/home/data0/dufan/MedicalCNNDenoising/data/40x40_Res_Full/'):   \n",
    "    # make working directory\n",
    "\n",
    "    if not os.path.exists(outputPath):\n",
    "        os.makedirs(outputPath)\n",
    "    \n",
    "    # Path names\n",
    "    tlPath = os.path.join(outputPath, 'trainingList.txt')\n",
    "    trainPath = os.path.join(outputPath, 'DnCNN_Train.prototxt')\n",
    "    testPath = os.path.join(outputPath, 'DnCNN_Test.prototxt')\n",
    "    solverPath = os.path.join(outputPath, 'DnCNN_Solver.prototxt')\n",
    "    \n",
    "    # Generate trainingList\n",
    "    with open(tlPath, 'w') as f:\n",
    "        for i in indices:\n",
    "            f.write(os.path.join(patchPath, 'trainingData_'+str(i)+'.h5') + '\\n')\n",
    "        f.close()\n",
    "    \n",
    "    # Generate training prototxt\n",
    "    GenerateNetwork.GenerateNetPrototxt(trainPath, tlPath, depth, caffe.TRAIN, 100, dim2=patchSize, nChannel=nChannel)\n",
    "    GenerateNetwork.GenerateNetPrototxt(testPath, tlPath, depth, caffe.TEST, 20, dim2=patchSize, nChannel=nChannel)\n",
    "    \n",
    "    # Generate solver prototxt\n",
    "    GenerateNetwork.GenerateADAMSolverPrototxt(solverPath, trainPath, os.path.join(outputPath, 'DnCNN'),\n",
    "                                               max_iter, snapshot=10000)\n",
    "    \n",
    "    # Train\n",
    "    solver = caffe.AdamSolver(solverPath)\n",
    "    solver.solve()\n",
    "    solver.net.save(os.path.join(outputPath, 'DnCNN.caffemodel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate the training data for cascaded networks in the low dose challenge\n",
    "# indices - a list containing all the indices of the datasets\n",
    "# prototxts - a list containing all the trained test prototxts\n",
    "# caffemodels - a list containing all the trained model weights\n",
    "# outputPath - output dir for the newly trained net\n",
    "# nPatchesPerLayer - number of patches per layer\n",
    "# rTopLayers - select the rTopLayers*nLayers with top denoising error \n",
    "# basePath - the path of the low dose challenge dataset\n",
    "def GenSeqTrainingDataLowDoseChallenge(indices, prototxts, caffemodels, outputPath, \n",
    "                                       nPatchesPerLayer=1000, rTopLayers = 0.05,\n",
    "                                       basePath='/home/data0/dufan/CT_images/'):\n",
    "    # make outputPath\n",
    "    if not os.path.exists(outputPath):\n",
    "        os.makedirs(outputPath)\n",
    "    \n",
    "    # load networks\n",
    "    nets = list()\n",
    "    for i in range(0,len(prototxts)):\n",
    "        nets.append(caffe.Net(prototxts[i], caffemodels[i], caffe.TEST))\n",
    "    \n",
    "    # Read all the high dose images and low dose images\n",
    "    for index in indices:\n",
    "        print 'Reading dataset %d...' %index,\n",
    "        hdImgs = ReadFromCavaderData.ReadFromLowdoseChallengeData(index, True)\n",
    "        ldImgs = ReadFromCavaderData.ReadFromLowdoseChallengeData(index, False)\n",
    "        print 'Done'\n",
    "        \n",
    "        print 'Denoising:'\n",
    "        for i in range(0,len(ldImgs)):\n",
    "            if i%50 == 0:\n",
    "                print '%d...' %i,\n",
    "            for j in range(0, len(nets)):\n",
    "                ldImgs[i] = ldImgs[i] - NNDenoise.PatchDenoiseParallel(nets[j], ldImgs[i])\n",
    "            hdImgs[i] = ldImgs[i] - hdImgs[i]\n",
    "        print 'Done'\n",
    "        \n",
    "        # get error list and select the indices\n",
    "        errList = np.zeros([len(hdImgs)])\n",
    "        for i in range(0,len(hdImgs)):\n",
    "            errList[i] = np.linalg.norm(hdImgs[i], ord='fro')\n",
    "        errIndices = np.argsort(errList)\n",
    "        errIndices = errIndices[::-1]\n",
    "        errIndices = errIndices[0:int(len(errIndices)*rTopLayers)]\n",
    "        print 'Selected %d layers with maximum error out of %d layers'%(len(errIndices), len(hdImgs))\n",
    "        print errIndices\n",
    "        \n",
    "        # patching\n",
    "        imgPatches = PatchingImgs(ldImgs, hdImgs, errIndices, nPatchesPerLayer)\n",
    "        \n",
    "        # output hdf5 file\n",
    "        print 'Writing hdf5...',\n",
    "        with h5py.File(os.path.join(outputPath, 'trainingData_'+str(index)+'.h5'), 'w') as f:\n",
    "            f['data'] = imgPatches.astype(np.float32)\n",
    "            f['label'] = np.zeros([imgPatches.shape[0]], dtype=np.float32)\n",
    "            f.close()\n",
    "        print 'Done'\n",
    "    \n",
    "    # generate trainingList\n",
    "    with open(os.path.join(outputPath, 'trainingList.txt'), 'w') as f:\n",
    "        for index in indices:\n",
    "            f.write(os.path.join(outputPath, 'trainingData_'+str(index)+'.h5\\n'))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basically the same with the genSeqTrain....\n",
    "# this one is for dual layer input\n",
    "def GenSeqTrainingDataLowDoseChallenge2(indices, prototxts, caffemodels, outputPath, \n",
    "                                       nPatchesPerLayer=1000, rTopLayers = 0.05,\n",
    "                                       basePath='/home/data0/dufan/CT_images/'):\n",
    "    # make outputPath\n",
    "    if not os.path.exists(outputPath):\n",
    "        os.makedirs(outputPath)\n",
    "    \n",
    "    # load networks\n",
    "    nets = list()\n",
    "    for i in range(0,len(prototxts)):\n",
    "        nets.append(caffe.Net(prototxts[i], caffemodels[i], caffe.TEST))\n",
    "    \n",
    "    # Read all the high dose images and low dose images\n",
    "    for index in indices:\n",
    "        print 'Reading dataset %d...' %index,\n",
    "        hdImgs = ReadFromCavaderData.ReadFromLowdoseChallengeData(index, True)\n",
    "        ldImgs = ReadFromCavaderData.ReadFromLowdoseChallengeData(index, False)\n",
    "        oriLdImgs = ReadFromCavaderData.ReadFromLowdoseChallengeData(index, False)\n",
    "        print 'Done'\n",
    "        \n",
    "        print 'Denoising:'\n",
    "        for i in range(0,len(ldImgs)):\n",
    "            if i%50 == 0:\n",
    "                print '%d...' %i,\n",
    "            for j in range(0, len(nets)):\n",
    "                if nets[j].blobs['dataSrc'].data.shape[1] == 1:\n",
    "                    ldImgs[i] = ldImgs[i] - NNDenoise.PatchDenoiseParallel(nets[j], ldImgs[i])\n",
    "                else:\n",
    "                    ldImgs[i] = ldImgs[i] - NNDenoise.PatchDenoiseParallel2(nets[j], ldImgs[i], oriLdImgs[i])\n",
    "            hdImgs[i] = ldImgs[i] - hdImgs[i]\n",
    "        print 'Done'\n",
    "        \n",
    "        # get error list and select the indices\n",
    "        errList = np.zeros([len(hdImgs)])\n",
    "        for i in range(0,len(hdImgs)):\n",
    "            errList[i] = np.linalg.norm(hdImgs[i], ord='fro')\n",
    "        errIndices = np.argsort(errList)\n",
    "        errIndices = errIndices[::-1]\n",
    "        errIndices = errIndices[0:int(len(errIndices)*rTopLayers)]\n",
    "        print 'Selected %d layers with maximum error out of %d layers'%(len(errIndices), len(hdImgs))\n",
    "        print errIndices\n",
    "        \n",
    "        # patching\n",
    "        imgPatches = PatchingImgs2(ldImgs, oriLdImgs, hdImgs, errIndices, nPatchesPerLayer)\n",
    "        \n",
    "        # output hdf5 file\n",
    "        print 'Writing hdf5...',\n",
    "        with h5py.File(os.path.join(outputPath, 'trainingData_'+str(index)+'.h5'), 'w') as f:\n",
    "            f['data'] = imgPatches.astype(np.float32)\n",
    "            f['label'] = np.zeros([imgPatches.shape[0]], dtype=np.float32)\n",
    "            f.close()\n",
    "        print 'Done'\n",
    "    \n",
    "    # generate trainingList\n",
    "    with open(os.path.join(outputPath, 'trainingList.txt'), 'w') as f:\n",
    "        for index in indices:\n",
    "            f.write(os.path.join(outputPath, 'trainingData_'+str(index)+'.h5\\n'))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
