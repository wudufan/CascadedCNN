{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "# import caffe\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libopencv_highgui.so.2.4: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e6bbda717791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sys.path.append('./PublicFunctions/')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPublicFunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadFromCavaderData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPublicFunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCadaverTrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# import ReadFromCavaderData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# import CadaverTrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data0/dufan/MedicalCNNDenoising/Example/PublicFunctions/CadaverTrain.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# In[1]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcaffe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadFromCavaderData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/caffe-master/python/caffe/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpycaffe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGDSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNesterovSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaGradSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRMSPropSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaDeltaSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamSolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_caffe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_mode_cpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_mode_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_type_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_caffe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaffe_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/caffe-master/python/caffe/pycaffe.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_caffe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGDSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNesterovSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaGradSolver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mRMSPropSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaDeltaSolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamSolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcaffe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libopencv_highgui.so.2.4: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from PublicFunctions import ReadFromCavaderData\n",
    "from PublicFunctions import CadaverTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = [0,1] # regarding dataset L067 and L096\n",
    "baseTrainDir = '/home/data0/dufan/MedicalCNNDenoising/Example/SampleTrain'\n",
    "if not os.path.exists(baseTrainDir):\n",
    "    os.makedirs(baseTrainDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate training patches\n",
    "# read the dicom images\n",
    "for i in indices:\n",
    "    print 'Patching %d'%i\n",
    "    imgh = ReadFromCavaderData.ReadFromLowdoseChallengeData(i, True, None, \n",
    "                                                            basePath='/home/data0/dufan/CT_images/')\n",
    "    imgl = ReadFromCavaderData.ReadFromLowdoseChallengeData(i, False)\n",
    "    \n",
    "    #  the ouput are lists, make imgh the residue for residue learning\n",
    "    for j in range(0, len(imgh)):\n",
    "        imgh[j] = imgl[j] - imgh[j]  # do no make the wrong order\n",
    "        \n",
    "    # generate image patches\n",
    "    imgPatches = CadaverTrain.PatchingImgs(imgl,imgh, nPatchesPerLayer=50)\n",
    "    \n",
    "    # random shuffle\n",
    "    inds = range(0,imgPatches.shape[0])\n",
    "    random.shuffle(inds)\n",
    "    imgPatches = imgPatches[inds, ...]\n",
    "    \n",
    "    # write hd5f files\n",
    "    with h5py.File(os.path.join(baseTrainDir, 'trainingData_%d.h5'%i), 'w') as f:\n",
    "        f['data'] = imgPatches.astype(np.float32)\n",
    "        f['label'] = np.zeros(imgPatches.shape[0], dtype=np.float32)\n",
    "        f.close()\n",
    "\n",
    "# write the trainingList.txt, not neccessary\n",
    "with open(os.path.join(baseTrainDir, 'trainingList.txt'), 'w') as f:\n",
    "    for i in indices:\n",
    "        f.write(os.path.join(baseTrainDir, 'trainingData_%d.h5\\n'%i)) # better use absolute path here\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(3)\n",
    "\n",
    "depth = 5\n",
    "\n",
    "# first round training\n",
    "print 'Iteration 0'\n",
    "trainPath = os.path.join(baseTrainDir, 'trainedNets')\n",
    "curPath = os.path.join(trainPath, '0')\n",
    "CadaverTrain.TrainLowDoseChallenge(indices, 5, curPath, max_iter=1000, \n",
    "                                  patchPath=baseTrainDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the cascaded training\n",
    "for i in range(1,4):\n",
    "    print 'Iteration %d'%i\n",
    "    curPath = os.path.join(trainPath, str(i))\n",
    "    \n",
    "    # make a list of all the past test prototxt files and caffemodel files\n",
    "    prototxts = list()\n",
    "    caffemodels = list()\n",
    "    for j in range(0,i):\n",
    "        prototxts.append(os.path.join(trainPath, str(j), 'DnCNN_Test.prototxt'))\n",
    "        caffemodels.append(os.path.join(trainPath, str(j), 'DnCNN.caffemodel'))\n",
    "    \n",
    "    # denoise the training dataset \n",
    "    # 2 for dual layer input, remove 2 for single layer input\n",
    "    CadaverTrain.GenSeqTrainingDataLowDoseChallenge2(indices, prototxts, caffemodels, curPath, \n",
    "                                                    nPatchesPerLayer=100, rTopLayers=0.05)\n",
    "    \n",
    "    print 'Training...',\n",
    "    # again, channel =2 for dual layer input\n",
    "    CadaverTrain.TrainLowDoseChallenge(indices, depth, curPath, 1000, nChannel=2,\n",
    "                                       patchPath=curPath)\n",
    "    print 'Done'\n",
    "\n",
    "print 'All Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
